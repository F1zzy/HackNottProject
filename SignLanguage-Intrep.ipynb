{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d25dc9d6-324c-41b3-b38f-dcbfda382838",
   "metadata": {},
   "source": [
    "# Text To Speech Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ae075-fb0b-40e8-ba68-dd26a822f5be",
   "metadata": {},
   "source": [
    "## Imports And Libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c71ffc6b-5004-4c56-9e87-620dc6acc562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- ----------------\n",
      "absl-py                      2.0.0\n",
      "aiohttp                      3.8.6\n",
      "aiosignal                    1.3.1\n",
      "anyio                        4.0.0\n",
      "apturl                       0.5.2\n",
      "argon2-cffi                  23.1.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.3.0\n",
      "asttokens                    2.4.1\n",
      "astunparse                   1.6.3\n",
      "async-lru                    2.0.4\n",
      "async-timeout                4.0.3\n",
      "attrs                        23.1.0\n",
      "Babel                        2.13.1\n",
      "beautifulsoup4               4.12.2\n",
      "bleach                       6.1.0\n",
      "blinker                      1.4\n",
      "Brlapi                       0.8.3\n",
      "cachetools                   5.3.1\n",
      "certifi                      2020.6.20\n",
      "cffi                         1.16.0\n",
      "chardet                      4.0.0\n",
      "charset-normalizer           3.3.0\n",
      "click                        8.0.3\n",
      "colorama                     0.4.4\n",
      "comm                         0.1.4\n",
      "command-not-found            0.3\n",
      "contourpy                    1.2.0\n",
      "cryptography                 3.4.8\n",
      "cupshelpers                  1.0\n",
      "cycler                       0.12.1\n",
      "dbus-python                  1.2.18\n",
      "debugpy                      1.8.0\n",
      "decorator                    5.1.1\n",
      "defer                        1.0.6\n",
      "defusedxml                   0.7.1\n",
      "discord.py                   2.3.2\n",
      "distro                       1.7.0\n",
      "distro-info                  1.1+ubuntu0.1\n",
      "exceptiongroup               1.1.3\n",
      "executing                    2.0.1\n",
      "fastjsonschema               2.18.1\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.44.0\n",
      "fqdn                         1.5.1\n",
      "frozenlist                   1.4.0\n",
      "gast                         0.4.0\n",
      "google-auth                  2.23.3\n",
      "google-auth-oauthlib         1.0.0\n",
      "google-pasta                 0.2.0\n",
      "greenlet                     3.0.0\n",
      "grpcio                       1.59.2\n",
      "gspread                      5.11.3\n",
      "gTTS                         2.4.0\n",
      "h5py                         3.10.0\n",
      "httplib2                     0.20.2\n",
      "idna                         3.3\n",
      "importlib-metadata           4.6.4\n",
      "intel_tensorflow             2.13.0\n",
      "ipykernel                    6.26.0\n",
      "ipython                      8.17.2\n",
      "isoduration                  20.11.0\n",
      "jedi                         0.19.1\n",
      "jeepney                      0.7.1\n",
      "Jinja2                       3.1.2\n",
      "joblib                       1.3.2\n",
      "json5                        0.9.14\n",
      "jsonpointer                  2.4\n",
      "jsonschema                   4.19.2\n",
      "jsonschema-specifications    2023.7.1\n",
      "jupyter_client               8.5.0\n",
      "jupyter_core                 5.5.0\n",
      "jupyter-events               0.8.0\n",
      "jupyter-lsp                  2.2.0\n",
      "jupyter_server               2.9.1\n",
      "jupyter_server_terminals     0.4.4\n",
      "jupyterlab                   4.0.8\n",
      "jupyterlab-pygments          0.2.2\n",
      "jupyterlab_server            2.25.0\n",
      "keras                        2.13.1\n",
      "keyring                      23.5.0\n",
      "kiwisolver                   1.4.5\n",
      "language-selector            0.1\n",
      "launchpadlib                 1.10.16\n",
      "lazr.restfulclient           0.14.4\n",
      "lazr.uri                     1.0.6\n",
      "libclang                     16.0.6\n",
      "louis                        3.20.0\n",
      "lxml                         4.9.3\n",
      "macaroonbakery               1.3.1\n",
      "Markdown                     3.5.1\n",
      "MarkupSafe                   2.1.3\n",
      "matplotlib                   3.8.1\n",
      "matplotlib-inline            0.1.6\n",
      "mediapipe                    0.10.7\n",
      "mistune                      3.0.2\n",
      "ml-dtypes                    0.2.0\n",
      "more-itertools               8.10.0\n",
      "multidict                    6.0.4\n",
      "nbclient                     0.8.0\n",
      "nbconvert                    7.10.0\n",
      "nbformat                     5.9.2\n",
      "nest-asyncio                 1.5.8\n",
      "netifaces                    0.11.0\n",
      "notebook                     7.0.6\n",
      "notebook_shim                0.2.3\n",
      "numpy                        1.24.3\n",
      "oauthlib                     3.2.0\n",
      "olefile                      0.46\n",
      "opencv-contrib-python        4.8.1.78\n",
      "opencv-python                4.8.1.78\n",
      "opt-einsum                   3.3.0\n",
      "overrides                    7.4.0\n",
      "packaging                    23.2\n",
      "pandas                       2.1.1\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.3\n",
      "pexpect                      4.8.0\n",
      "Pillow                       9.0.1\n",
      "pip                          22.0.2\n",
      "platformdirs                 3.11.0\n",
      "playsound                    1.3.0\n",
      "playwright                   1.39.0\n",
      "prometheus-client            0.18.0\n",
      "prompt-toolkit               3.0.39\n",
      "protobuf                     3.20.3\n",
      "psutil                       5.9.6\n",
      "ptyprocess                   0.7.0\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.0\n",
      "pyasn1-modules               0.3.0\n",
      "pycairo                      1.20.1\n",
      "pycparser                    2.21\n",
      "pycups                       2.0.1\n",
      "pyee                         11.0.1\n",
      "Pygments                     2.16.1\n",
      "PyGObject                    3.42.1\n",
      "PyJWT                        2.3.0\n",
      "pymacaroons                  0.13.0\n",
      "PyNaCl                       1.5.0\n",
      "pyparsing                    2.4.7\n",
      "pyRFC3339                    1.1\n",
      "python-apt                   2.4.0+ubuntu2\n",
      "python-dateutil              2.8.2\n",
      "python-debian                0.1.43+ubuntu1.1\n",
      "python-json-logger           2.0.7\n",
      "python-version               0.0.2\n",
      "pyttsx3                      2.90\n",
      "pytz                         2022.1\n",
      "pyxdg                        0.27\n",
      "PyYAML                       5.4.1\n",
      "pyzmq                        25.1.1\n",
      "referencing                  0.30.2\n",
      "reportlab                    3.6.8\n",
      "requests                     2.31.0\n",
      "requests-oauthlib            1.3.1\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rpds-py                      0.12.0\n",
      "rsa                          4.9\n",
      "scikit-learn                 1.3.2\n",
      "scipy                        1.11.3\n",
      "screen-resolution-extra      0.0.0\n",
      "SecretStorage                3.3.1\n",
      "Send2Trash                   1.8.2\n",
      "setuptools                   59.6.0\n",
      "six                          1.16.0\n",
      "sklearn                      0.0.post10\n",
      "sniffio                      1.3.0\n",
      "sounddevice                  0.4.6\n",
      "soupsieve                    2.5\n",
      "stack-data                   0.6.3\n",
      "systemd-python               234\n",
      "tensorboard                  2.13.0\n",
      "tensorboard-data-server      0.7.2\n",
      "tensorflow                   2.14.0\n",
      "tensorflow-estimator         2.13.0\n",
      "tensorflow-io-gcs-filesystem 0.34.0\n",
      "termcolor                    2.3.0\n",
      "terminado                    0.17.1\n",
      "threadpoolctl                3.2.0\n",
      "tinycss2                     1.2.1\n",
      "tomli                        2.0.1\n",
      "tornado                      6.3.3\n",
      "traitlets                    5.13.0\n",
      "types-python-dateutil        2.8.19.14\n",
      "typing_extensions            4.5.0\n",
      "tzdata                       2023.3\n",
      "ubuntu-advantage-tools       8001\n",
      "ubuntu-drivers-common        0.0.0\n",
      "ufw                          0.36.1\n",
      "unattended-upgrades          0.1\n",
      "uri-template                 1.3.0\n",
      "urllib3                      1.26.5\n",
      "wadllib                      1.3.6\n",
      "wcwidth                      0.2.9\n",
      "webcolors                    1.13\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.6.4\n",
      "Werkzeug                     3.0.1\n",
      "wheel                        0.37.1\n",
      "wrapt                        1.14.1\n",
      "xdg                          5\n",
      "xkit                         0.0.0\n",
      "xvfbwrapper                  0.2.9\n",
      "yarl                         1.9.2\n",
      "zipp                         1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "    pip list # All The Pip installs have been made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc094b5-9d06-49ae-957a-edba534148a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 05:36:20.370136: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as pltlist_physical_devices\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ec7a79-913e-451d-9904-cb48097529ad",
   "metadata": {},
   "source": [
    "## Keypoints and values for evulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8fe369c-f5ec-4af6-9f9c-d790fe049617",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic #Skeleton Model,\n",
    "mp_drawing = mp.solutions.drawing_utils #Drawing Points On the Skeleton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d6e022-224e-4fcb-918f-868ea11b8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Allow to convert Colour of image going from BGR to RGC\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction on image \n",
    "    image.flags.writeable = True                   # Then Write to Image \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #Going Back to BGR\n",
    "    return image, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23bfe7e6-c350-437b-8bba-b2ea5eb47e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Right Hand  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49278f2a-99be-447a-8fc5-38694b52b1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "#Creating Camera for Testing\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Using Full resolution of caemra.\n",
    "#Set FPS\n",
    "fps_time = time.time()\n",
    "#Set Mediapipe Model To start tracking and then Go off that Point \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened:\n",
    "        #Reading video\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        #Making Detections:\n",
    "        image, results = mediapipe_detection(frame,holistic)\n",
    "        #Draw Sketelon \n",
    "        draw_landmarks(image,results)\n",
    "        \n",
    "        # Calculate FPS\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - fps_time)\n",
    "        fps_time = current_time\n",
    "    \n",
    "        # Display the FPS on the video feed\n",
    "        cv2.putText(image, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        #Show on Laptop\n",
    "        cv2.imshow('Feed' , image)\n",
    "        #Quit When Q is pressed ( Hopefully Gracefully) \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef3c75-46ab-45b6-9c06-da0c72e22a72",
   "metadata": {},
   "source": [
    "## Getting Values From MediaPipe to Train & Create Folders for data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d098530-f841-4f3e-9dd4-a9858eac786a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34020caf-bba0-4ea3-a192-117ef5c5bd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64007725-b896-4a41-8f20-ffd8b2b52288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806f487e-143c-467d-985d-45d20d4019a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "rhsize = 21*3\n",
    "print(rhsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17b124e-44d3-47f9-b0c9-b1509f4e18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results): # Extract The X Y Z Values of All the Points. If not on screen Replace with 0\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(rhsize)\n",
    "    return np.concatenate([rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7090c9e-eb14-4a3e-9c4a-5081d01315e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "if(rhsize == len(extract_keypoints(results))):\n",
    "    print(len(extract_keypoints(results)))\n",
    "else:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec5df60a-e041-4acd-b5e8-7ecffb231436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data')\n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['Hello' , 'I Love You' , 'Good' ,\"Bad\" , 'Yes', 'No'])\n",
    "no_sequences = 30\n",
    "\n",
    "#30 Frames ( 10 FPS Video - 2 Secs A Vid)\n",
    "sequence_length = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db7eb08f-b3f1-4f0e-a34e-ccad52e1c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a Folder for each Word\n",
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74ca65-d2b6-4e1d-b49b-773fecf17f66",
   "metadata": {},
   "source": [
    "## Making Videos For Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e7a3d7-c862-4a33-ab8a-fe7849af8be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb9dbee-baad-41c1-86cc-1114f6f1e155",
   "metadata": {},
   "source": [
    "# # Using Data Collected For For Machine LEearning to create Labels and Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62edad23-757e-4897-8c9b-78d5a8c14f61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Wait for spacebar press to start the action collection\n",
    "        cv2.putText(image, 'Press SPACE to start collecting frames for {}'.format(action), (15, 12), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        \n",
    "        while True:\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord(' '):  # Spacebar press\n",
    "                break\n",
    "                \n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "#                 print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeac891-774c-4f46-9aa9-2cf86bca07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3783544-8f1d-4dfd-b4c6-0e903308d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9976f6e5-3875-425e-ae7d-64d2cbd4bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preforcessing Imports \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e2925-b78e-4559-bf37-9cea49bf2fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbac5ee7-9411-46c3-8407-e6514f10a934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_test: (108, 6)\n"
     ]
    }
   ],
   "source": [
    "# Define the actions you want to include in the training data\n",
    "selected_actions = actions\n",
    "\n",
    "# Create a label map for the selected actions\n",
    "label_map = {label: num for num, label in enumerate(selected_actions)}\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for action in selected_actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "# Convert sequences and labels to NumPy arrays\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6)\n",
    "\n",
    "# Check the shape of the test labels\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c5f6f-98b1-4f84-a0fe-e94f45a4b141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77156326-b823-4f56-a759-9ff9874a98a5",
   "metadata": {},
   "source": [
    "## Train Neutral network Then predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d3d68b8-9424-4429-8bed-53ffd7adfb86",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Assuming you have already prepared your X_train and y_train using the filtered dataset\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[tb_callback])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Display a summary of the model architecture\u001b[39;00m\n\u001b[1;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming you've already filtered your dataset to include only the first 3 actions\n",
    "# Define your log directory for TensorBoard\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add LSTM layers\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(20, 63)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Output layer with activation 'softmax'; use the number of classes in your dataset\n",
    "# In your case, it's the number of unique actions in the filtered dataset\n",
    "num_classes = len(selected_actions)\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Assuming you have already prepared your X_train and y_train using the filtered dataset\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])\n",
    "\n",
    "# Display a summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ff50a-1d41-44fc-80c1-c4fb5c8cb04e",
   "metadata": {},
   "source": [
    " ## Make Prediction and Save Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e60adce-5bb7-430f-b27c-0142f0e0d658",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/f1z/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('0.5Acc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "612ee43b-93f3-4b24-90f4-95ec2335b217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 54ms/step\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "\n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n",
    "multilabel_confusion_matrix(ytrue, yhat)\n",
    "print(accuracy_score(ytrue, yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90c9d5b3-0fa9-4d73-9029-14c5076b2e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 05:37:05.817053: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('RHAcc0.84.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2107a431-c4d1-4720-8478-4eafd93ecd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 58ms/step\n",
      "Accuracy Score: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have trained a neural network model 'model' and made predictions 'y_pred'\n",
    "y_true = np.argmax(y_test, axis=1)  # If your y_test is one-hot encoded, convert it to class indices\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert model predictions to class indices\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "print(\"Accuracy Score:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c2c8c-f798-44cc-938f-3a3ca390370b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5fabb-920e-417f-a4cf-977b87531a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a37afe-ec23-4a21-b0a2-d5409bbba647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9646c63-43f0-4ec2-9d7c-64efbd6a6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        # Define the coordinates of the top-left and bottom-right corners\n",
    "        top_left = (0, 60 + num * 40)\n",
    "        bottom_right = (int(prob * 100), 90 + num * 40)\n",
    "        # Draw the rectangle using the defined coordinates\n",
    "        cv2.rectangle(output_frame, top_left, bottom_right, (255,255,255), -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85 + num * 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "820ea330-0421-46e1-b3d8-6cd4e3144df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import playsound\n",
    "\n",
    "def text_to_speech(text):\n",
    "    # Convert the input text to speech\n",
    "    tts = gTTS(text)\n",
    "\n",
    "    # Save the speech as an MP3 file\n",
    "    tts.save(\"output.mp3\")\n",
    "\n",
    "    # Play the audio file\n",
    "    playsound.playsound(\"output.mp3\")\n",
    "\n",
    "    # Optionally, you can remove the temporary audio file after playing\n",
    "    # os.remove(\"output.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4d29b6a-d377-493f-9534-a2b84354d03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 1s 676ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Yes\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Yes\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Yes\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Yes\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "Good\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "I Love You\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "Hello\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.8\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "#         sequence.insert(0,keypoints)\n",
    "#         sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                        text_to_speech(action)\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "                    text_to_speech(action)\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            if res[np.argmax(res)] > threshold:\n",
    "                action = actions[np.argmax(res)]\n",
    "            if len(sentence) > 0:\n",
    "                if action != sentence[-1]:\n",
    "                    sentence.append(action)\n",
    "                    text_to_speech(action)\n",
    "            else:\n",
    "                sentence.append(action)\n",
    "                text_to_speech(action)\n",
    "    \n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5]\n",
    "    \n",
    "            # Viz probabilities\n",
    "            colors = (255,255,255)\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "    \n",
    "            # Convert the recognized action to speech\n",
    "            \n",
    "\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8c1eb01b-e23c-4204-b0c2-49836f8285db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gTTS in /home/f1z/.local/lib/python3.10/site-packages (2.4.0)\n",
      "Collecting playsound\n",
      "  Downloading playsound-1.3.0.tar.gz (7.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click<8.2,>=7.1 in /usr/lib/python3/dist-packages (from gTTS) (8.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /home/f1z/.local/lib/python3.10/site-packages (from gTTS) (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.27->gTTS) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.27->gTTS) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/f1z/.local/lib/python3.10/site-packages (from requests<3,>=2.27->gTTS) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.27->gTTS) (3.3)\n",
      "Building wheels for collected packages: playsound\n",
      "  Building wheel for playsound (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for playsound: filename=playsound-1.3.0-py3-none-any.whl size=7035 sha256=49111c9775ec8967b9c72f621820e526f9b2df046eb66a93549c239a0f6002e7\n",
      "  Stored in directory: /home/f1z/.cache/pip/wheels/90/89/ed/2d643f4226fc8c7c9156fc28abd8051e2d2c0de37ae51ac45c\n",
      "Successfully built playsound\n",
      "Installing collected packages: playsound\n",
      "Successfully installed playsound-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gTTS playsound"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
